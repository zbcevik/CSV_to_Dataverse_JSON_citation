# -*- coding: utf-8 -*-
"""csv_to_dataverse_json.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s0blCFrgpFvF20uC_MTgjuPd3k4iov8g
"""

import csv
import json
import re
import pandas as pd
from datetime import datetime

def csv_to_dataverse_json(csv_file_path, output_json_path):
    """
    Convert CSV file to complete Dataverse JSON format.
    Includes all top-level fields and metadata blocks.
    """

    # Field type directory - defines structure for all citation fields
    directory = {
        'title': {"typeName": "title", "multiple": False, "typeClass": "primitive"},
        'subtitle': {"typeName": "subtitle", "multiple": False, "typeClass": "primitive"},
        'alternativeTitle': {"typeName": "alternativeTitle", "multiple": True, "typeClass": "primitive"},
        'otherId': {"typeName": "otherId", "multiple": True, "typeClass": "compound"},
        'author': {"typeName": "author", "multiple": True, "typeClass": "compound"},
        'datasetContact': {"typeName": "datasetContact", "multiple": True, "typeClass": "compound"},
        'dsDescription': {"typeName": "dsDescription", "multiple": True, "typeClass": "compound"},
        'subject': {"typeName": "subject", "multiple": True, "typeClass": "controlledVocabulary"},
        'keyword': {"typeName": "keyword", "multiple": True, "typeClass": "compound"},
        'topicClassification': {"typeName": "topicClassification", "multiple": True, "typeClass": "compound"},
        'publication': {"typeName": "publication", "multiple": True, "typeClass": "compound"},
        'notesText': {"typeName": "notesText", "multiple": False, "typeClass": "primitive"},
        'language': {"typeName": "language", "multiple": True, "typeClass": "controlledVocabulary"},
        'producer': {"typeName": "producer", "multiple": True, "typeClass": "compound"},
        'productionDate': {"typeName": "productionDate", "multiple": False, "typeClass": "primitive"},
        'productionPlace': {"typeName": "productionPlace", "multiple": True, "typeClass": "primitive"},
        'contributor': {"typeName": "contributor", "multiple": True, "typeClass": "compound"},
        'grantNumber': {"typeName": "grantNumber", "multiple": True, "typeClass": "compound"},
        'distributor': {"typeName": "distributor", "multiple": True, "typeClass": "compound"},
        'distributionDate': {"typeName": "distributionDate", "multiple": False, "typeClass": "primitive"},
        'depositor': {"typeName": "depositor", "multiple": False, "typeClass": "primitive"},
        'dateOfDeposit': {"typeName": "dateOfDeposit", "multiple": False, "typeClass": "primitive"},
        'timePeriodCovered': {"typeName": "timePeriodCovered", "multiple": True, "typeClass": "compound"},
        'dateOfCollection': {"typeName": "dateOfCollection", "multiple": True, "typeClass": "compound"},
        'kindOfData': {"typeName": "kindOfData", "multiple": True, "typeClass": "primitive"},
        'series': {"typeName": "series", "multiple": True, "typeClass": "compound"},
        'software': {"typeName": "software", "multiple": True, "typeClass": "compound"},
        'relatedMaterial': {"typeName": "relatedMaterial", "multiple": True, "typeClass": "primitive"},
        'relatedDatasets': {"typeName": "relatedDatasets", "multiple": True, "typeClass": "primitive"},
        'otherReferences': {"typeName": "otherReferences", "multiple": True, "typeClass": "primitive"},
        'dataSources': {"typeName": "dataSources", "multiple": True, "typeClass": "primitive"},
        'originOfSources': {"typeName": "originOfSources", "multiple": False, "typeClass": "primitive"},
        'characteristicOfSources': {"typeName": "characteristicOfSources", "multiple": False, "typeClass": "primitive"},
        'accessToSources': {"typeName": "accessToSources", "multiple": False, "typeClass": "primitive"}
    }

    # Compound field subfield mappings
    compound_fields = {
        'otherId': ['otherIdAgency', 'otherIdValue'],
        'author': ['authorName', 'authorAffiliation', 'authorIdentifierScheme', 'authorIdentifier'],
        'datasetContact': ['datasetContactName', 'datasetContactAffiliation', 'datasetContactEmail'],
        'dsDescription': ['dsDescriptionValue', 'dsDescriptionDate'],
        'keyword': ['keywordValue', 'keywordVocabulary', 'keywordVocabularyURI'],
        'topicClassification': ['topicClassValue', 'topicClassVocab', 'topicClassVocabURI'],
        'publication': ['publicationCitation', 'publicationIDType', 'publicationIDNumber', 'publicationURL'],
        'producer': ['producerName', 'producerAffiliation', 'producerAbbreviation', 'producerURL', 'producerLogoURL'],
        'contributor': ['contributorType', 'contributorName'],
        'grantNumber': ['grantNumberAgency', 'grantNumberValue'],
        'distributor': ['distributorName', 'distributorAffiliation', 'distributorAbbreviation', 'distributorURL', 'distributorLogoURL'],
        'timePeriodCovered': ['timePeriodCoveredStart', 'timePeriodCoveredEnd'],
        'dateOfCollection': ['dateOfCollectionStart', 'dateOfCollectionEnd'],
        'series': ['seriesName', 'seriesInformation'],
        'software': ['softwareName', 'softwareVersion']
    }

    # Get current date for defaults
    current_date = datetime.now().strftime("%Y-%m-%d")
    current_year = datetime.now().strftime("%Y")

    # Helper function to convert dates to year-only format
    def format_date_to_year(date_value):
        """Convert any date format to YYYY format"""
        if pd.isna(date_value) or not date_value:
            return current_year

        date_str = str(date_value).strip()

        # Extract year using regex
        year_match = re.search(r'\b(19|20)\d{2}\b', date_str)
        if year_match:
            return year_match.group(0)

        return current_year

    # Read CSV file and process each row
    df = pd.read_csv(csv_file_path)
    all_datasets = []

    for idx, row in df.iterrows():
        # Build JSON structure focused on citation metadata
        dataset_json = {
            "metadataBlocks": {
                "citation": {
                    "displayName": "Citation Metadata",
                    "name": "citation",
                    "fields": []
                }
            }
        }

        fields = dataset_json["metadataBlocks"]["citation"]["fields"]

        # Process each metadata field
        for field_name, field_config in directory.items():
            if field_name not in row or pd.isna(row[field_name]) or row[field_name] == "":
                continue

            value = str(row[field_name]).strip()
            if not value:
                continue

            # Build field structure
            field_entry = {
                "typeName": field_config["typeName"],
                "multiple": field_config["multiple"],
                "typeClass": field_config["typeClass"]
            }

            # Process based on type
            if field_config["typeClass"] == "primitive":
                # Convert date fields to year-only format
                if field_name in ['productionDate', 'distributionDate', 'dateOfDeposit']:
                    value = format_date_to_year(value)
                if field_config["multiple"]:
                    # Multiple primitive: split by pipe
                    field_entry["value"] = [v.strip() for v in value.split('|') if v.strip()]
                else:
                    # Single primitive
                    field_entry["value"] = value

            elif field_config["typeClass"] == "controlledVocabulary":
                # Controlled vocabulary: split by pipe
                field_entry["value"] = [v.strip() for v in value.split('|') if v.strip()]

            elif field_config["typeClass"] == "compound":
                # Compound: parse with subfields
                field_entry["value"] = parse_compound(value, field_name, compound_fields)

            # Add to fields list
            if field_entry.get("value"):
                fields.append(field_entry)

        # Add geospatial metadata block if present
        if any(col in row for col in ['geographicCoverage', 'geographicUnit', 'geographicBoundingBox']):
            geo_block = create_geospatial_block(row)
            if geo_block:
                dataset_json["metadataBlocks"]["geospatial"] = geo_block

        # Add social science metadata block if present
        if any(col in row for col in ['unitOfAnalysis', 'universe', 'timeMethod', 'samplingProcedure']):
            social_block = create_socialscience_block(row)
            if social_block:
                dataset_json["metadataBlocks"]["socialscience"] = social_block

        all_datasets.append(dataset_json)
        print(f"✓ Row {idx + 1}: Processed {len(fields)} citation fields")

    # Write output JSON file
    # If single row, write as single object; if multiple rows, write as array
    output_data = all_datasets[0] if len(all_datasets) == 1 else all_datasets

    with open(output_json_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)

    print(f"\n✓ Successfully converted CSV to JSON: {output_json_path}")
    print(f"✓ Total rows processed: {len(all_datasets)}")
    return output_data

def parse_compound(value, field_name, compound_fields):
    """
    Parse compound field values.
    Format: "value1; value2; value3 | value1; value2; value3"
    """
    if field_name not in compound_fields:
        return []

    subfield_names = compound_fields[field_name]
    entries = [e.strip() for e in value.split('|') if e.strip()]
    result = []

    for entry in entries:
        parts = [p.strip() for p in entry.split(';')]
        entry_obj = {}

        for i, subfield in enumerate(subfield_names):
            if i < len(parts) and parts[i] and parts[i].lower() != 'nan':
                # Special handling for dsDescriptionDate - convert to year
                if subfield == 'dsDescriptionDate':
                    year_match = re.search(r'\b(19|20)\d{2}\b', parts[i])
                    if year_match:
                        parts[i] = year_match.group(0)
                    else:
                        continue  # Skip invalid dates

                entry_obj[subfield] = {
                    "typeName": subfield,
                    "multiple": False,
                    "typeClass": "primitive",
                    "value": parts[i]
                }

        if entry_obj:
            result.append(entry_obj)

    return result

def create_geospatial_block(row):
    """Create geospatial metadata block if fields are present."""
    geospatial = {
        "displayName": "Geospatial Metadata",
        "name": "geospatial",
        "fields": []
    }

    # Geographic Coverage
    if 'geographicCoverage' in row and row['geographicCoverage'] and not pd.isna(row['geographicCoverage']):
        countries = [c.strip() for c in str(row['geographicCoverage']).split('|') if c.strip()]
        geospatial["fields"].append({
            "typeName": "geographicCoverage",
            "multiple": True,
            "typeClass": "compound",
            "value": [{"country": {"typeName": "country", "multiple": False,
                                   "typeClass": "controlledVocabulary", "value": c}} for c in countries]
        })

    # Geographic Unit
    if 'geographicUnit' in row and row['geographicUnit'] and not pd.isna(row['geographicUnit']):
        units = [u.strip() for u in str(row['geographicUnit']).split('|') if u.strip()]
        geospatial["fields"].append({
            "typeName": "geographicUnit",
            "multiple": True,
            "typeClass": "primitive",
            "value": units
        })

    return geospatial if geospatial["fields"] else None

def create_socialscience_block(row):
    """Create social science metadata block if fields are present."""
    socialscience = {
        "displayName": "Social Science and Humanities Metadata",
        "name": "socialscience",
        "fields": []
    }

    # Simple fields mapping
    simple_fields = {
        'unitOfAnalysis': {'multiple': True, 'typeClass': 'primitive'},
        'universe': {'multiple': True, 'typeClass': 'primitive'},
        'timeMethod': {'multiple': False, 'typeClass': 'primitive'},
        'frequencyOfDataCollection': {'multiple': False, 'typeClass': 'primitive'},
        'samplingProcedure': {'multiple': False, 'typeClass': 'primitive'},
        'collectionMode': {'multiple': True, 'typeClass': 'primitive'},
        'dataCollectionSituation': {'multiple': False, 'typeClass': 'primitive'},
        'weighting': {'multiple': False, 'typeClass': 'primitive'}
    }

    for field_name, config in simple_fields.items():
        if field_name in row and row[field_name] and not pd.isna(row[field_name]):
            field_entry = {
                "typeName": field_name,
                "multiple": config['multiple'],
                "typeClass": config['typeClass']
            }

            if config['multiple']:
                field_entry["value"] = [v.strip() for v in str(row[field_name]).split('|') if v.strip()]
            else:
                field_entry["value"] = str(row[field_name]).strip()

            socialscience["fields"].append(field_entry)

    return socialscience if socialscience["fields"] else None

'''
def create_template_csv(output_path='template.csv'):
    """
    Create a template CSV file with all field headers including top-level fields.
    """
    headers = [
        # Top-level fields
        'id', 'identifier', 'persistentUrl', 'protocol', 'authority', 'publisher',
        'publicationDate', 'storageIdentifier', 'datasetType',
        'versionId', 'datasetId', 'datasetPersistentId', 'versionNumber', 'versionMinorNumber',
        'versionState', 'latestVersionPublishingState', 'UNF', 'lastUpdateTime', 'releaseTime',
        'createTime', 'citationDate', 'termsOfUse', 'citationRequirements', 'conditions',
        'termsOfAccess', 'fileAccessRequest', 'citation',
        # Citation metadata fields
        'title', 'subtitle', 'alternativeTitle', 'otherId', 'author',
        'datasetContact', 'dsDescription', 'subject', 'keyword',
        'topicClassification', 'publication', 'notesText', 'language',
        'producer', 'productionDate', 'productionPlace', 'contributor',
        'grantNumber', 'distributor', 'distributionDate', 'depositor',
        'dateOfDeposit', 'timePeriodCovered', 'dateOfCollection',
        'kindOfData', 'series', 'software', 'relatedMaterial',
        'relatedDatasets', 'otherReferences', 'dataSources',
        'originOfSources', 'characteristicOfSources', 'accessToSources',
        # Geospatial fields
        'geographicCoverage', 'geographicUnit',
        # Social science fields
        'unitOfAnalysis', 'universe', 'timeMethod', 'frequencyOfDataCollection',
        'samplingProcedure', 'collectionMode', 'dataCollectionSituation', 'weighting'
    ]

    # Create sample data (only required fields)
    sample_data = {
        'title': 'Sample Dataset Title',
        'author': 'John Smith; Harvard University | Jane Doe; MIT',
        'datasetContact': 'Contact Person; University; contact@email.com',
        'dsDescription': 'This is a sample dataset description; 2024-01-01',
        'subject': 'Social Sciences | Medicine, Health and Life Sciences',
        'keyword': 'sample; | test; | data',
        'language': 'English',
        'protocol': 'doi',
        'datasetType': 'dataset',
        'versionNumber': '1',
        'versionMinorNumber': '0',
        'versionState': 'DRAFT'
    }

    df = pd.DataFrame([sample_data])
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"✓ Template CSV created: {output_path}")
    '''

# Main execution
if __name__ == "__main__":

    # Convert CSV to JSON
    # Update these paths as needed for your workspace
    csv_input = 'Csv_to_json - Citation.csv'
    json_output = 'output_metadata.json'
    
    csv_to_dataverse_json(csv_input, json_output)